#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¾é£Ÿç¬”è®°æœç´¢ - Flaskåç«¯API
è§£å†³å‰ç«¯CORSè·¨åŸŸé—®é¢˜
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
import requests
import logging
import time
import sqlite3
import os

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)  # å…è®¸æ‰€æœ‰æ¥æºçš„è·¨åŸŸè¯·æ±‚

# è¯·æ±‚å¤´æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
def get_headers(referer='https://www.douguo.com/', site='douguo'):
    """
    ç”Ÿæˆè¯·æ±‚å¤´ï¼Œæ¯æ¬¡è°ƒç”¨è¿”å›æ–°çš„headers
    
    å‚æ•°:
    - referer: æ¥æºé¡µé¢URL
    - site: ç›®æ ‡ç½‘ç«™ç±»å‹ ('baidu' æˆ– 'douguo')
    """
    # ç™¾åº¦æœç´¢ä¸èƒ½ä½¿ç”¨brå‹ç¼©ï¼Œä¼šå¯¼è‡´ä¹±ç 
    # è±†æœç¾é£Ÿå¯ä»¥ä½¿ç”¨brå‹ç¼©
    accept_encoding = 'gzip, deflate' if site == 'baidu' else 'gzip, deflate, br'
    
    return {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': accept_encoding,
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Cache-Control': 'max-age=0',
        'Sec-Ch-Ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
        'Sec-Ch-Ua-Mobile': '?0',
        'Sec-Ch-Ua-Platform': '"macOS"',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Sec-Fetch-User': '?1',
        'Referer': referer,
        'DNT': '1',
    }

@app.route('/')
def index():
    """APIé¦–é¡µ"""
    return jsonify({
        'status': 'ok',
        'message': 'ç¾é£Ÿç¬”è®°æœç´¢APIæœåŠ¡æ­£åœ¨è¿è¡Œ',
        'endpoints': {
            '/api/search-notes': 'æœç´¢ç¾é£Ÿç¬”è®°',
            '/api/health': 'å¥åº·æ£€æŸ¥'
        }
    })

@app.route('/api/health')
def health():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return jsonify({'status': 'healthy', 'service': 'ç¾é£Ÿç¬”è®°æœç´¢API'})

@app.route('/api/geocode', methods=['GET'])
def geocode():
    """
    é€†åœ°ç†ç¼–ç æ¥å£ï¼ˆç»çº¬åº¦ â†’ è¯¦ç»†åœ°å€ï¼‰
    ä½¿ç”¨é«˜å¾·åœ°å›¾API
    
    å‚æ•°ï¼š
    - lat: çº¬åº¦
    - lon: ç»åº¦
    
    è¿”å›ï¼šJSONæ ¼å¼çš„è¯¦ç»†åœ°å€ä¿¡æ¯
    """
    try:
        lat = request.args.get('lat', '').strip()
        lon = request.args.get('lon', '').strip()
        
        if not lat or not lon:
            return jsonify({'status': -1, 'message': 'ç¼ºå°‘ç»çº¬åº¦å‚æ•°'}), 400
        
        # ä½¿ç”¨é«˜å¾·åœ°å›¾é€†åœ°ç†ç¼–ç APIï¼ˆå›½å†…æœåŠ¡ï¼Œç¨³å®šå¿«é€Ÿï¼‰
        # æ³¨æ„ï¼šéœ€è¦ç”³è¯·é«˜å¾·åœ°å›¾WebæœåŠ¡API key
        # å¯ä»¥åœ¨ https://console.amap.com/ å…è´¹ç”³è¯·ï¼Œæ¯å¤©é…é¢å……è¶³
        amap_key = 'a9e44f7c387c1b48ac79da8e40fc716f'  # è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹keyï¼Œå»ºè®®æ›¿æ¢ä¸ºè‡ªå·±çš„
        api_url = f'https://restapi.amap.com/v3/geocode/regeo?key={amap_key}&location={lon},{lat}&extensions=all&output=json'
        
        logger.info(f"è¯·æ±‚é«˜å¾·é€†åœ°ç†ç¼–ç API: lat={lat}, lon={lon}")
        
        response = requests.get(api_url, timeout=10)
        response.raise_for_status()
        data = response.json()
        
        logger.info(f"é€†åœ°ç†ç¼–ç åŸå§‹å“åº”: status={data.get('status')}, info={data.get('info')}")
        
        if data.get('status') == '1' and data.get('regeocode'):
            regeocode = data['regeocode']
            formatted_address = regeocode.get('formatted_address', '')
            addressComponent = regeocode.get('addressComponent', {})
            
            # æå–åœ°å€ç»„ä»¶
            province = addressComponent.get('province', '')
            city = addressComponent.get('city', '') or addressComponent.get('province', '')  # ç›´è¾–å¸‚çš„cityä¸º[]
            district = addressComponent.get('district', '')
            township = addressComponent.get('township', '')
            street = addressComponent.get('streetNumber', {}).get('street', '')
            number = addressComponent.get('streetNumber', {}).get('number', '')
            
            # æ„å»ºè¯¦ç»†åœ°å€
            if not formatted_address:
                formatted_address = f"{province}{city}{district}{township}{street}{number}"
            
            result = {
                'status': 0,
                'message': 'success',
                'content': {
                    'address': formatted_address,
                    'address_detail': {
                        'province': province,
                        'city': city,
                        'district': district,
                        'street': township,
                        'streetNumber': f"{street}{number}",
                    },
                    'point': {
                        'x': lon,
                        'y': lat
                    }
                },
                'source': 'gps+amap'
            }
            logger.info(f"âœ… é€†åœ°ç†ç¼–ç æˆåŠŸ: {formatted_address}")
            return jsonify(result), 200
        else:
            error_msg = data.get('info', 'é€†åœ°ç†ç¼–ç å¤±è´¥')
            logger.error(f"é€†åœ°ç†ç¼–ç å¤±è´¥: status={data.get('status')}, info={error_msg}")
            return jsonify({'status': -1, 'message': error_msg}), 400
            
    except Exception as e:
        logger.error(f"é€†åœ°ç†ç¼–ç é”™è¯¯: {str(e)}")
        return jsonify({'status': -1, 'message': f'æœåŠ¡å™¨é”™è¯¯: {str(e)}'}), 500

@app.route('/api/ip-location', methods=['GET'])
def ip_location():
    """
    IPå®šä½ä»£ç†æ¥å£ï¼ˆä½¿ç”¨ ip-api.comï¼Œå…è´¹ä¸”ç¨³å®šï¼‰
    
    è¿”å›ï¼šJSONæ ¼å¼çš„ä½ç½®ä¿¡æ¯
    """
    try:
        # ä½¿ç”¨ ip-api.com å…è´¹æœåŠ¡ï¼ˆæ¯åˆ†é’Ÿ45æ¬¡è¯·æ±‚é™åˆ¶ï¼‰
        # lang=zh-CN å‚æ•°ç¡®ä¿è¿”å›ä¸­æ–‡åœ°å
        api_url = 'http://ip-api.com/json/?lang=zh-CN&fields=status,message,country,regionName,city,district,lat,lon,query'
        
        logger.info(f"è¯·æ±‚IPå®šä½API: {api_url}")
        
        # å‘èµ·è¯·æ±‚
        response = requests.get(
            api_url,
            timeout=10,
            headers={
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
                'Accept': 'application/json',
            }
        )
        
        response.raise_for_status()
        data = response.json()
        
        logger.info(f"IPå®šä½åŸå§‹å“åº”: {data}")
        
        if data.get('status') == 'success':
            province = data.get('regionName', '')
            city = data.get('city', '')
            district = data.get('district', '')
            
            # å¤„ç†åœ°åï¼šä¼˜å…ˆä½¿ç”¨çœä»½ï¼Œå¦‚æœåŸå¸‚æ˜¯è‹±æ–‡åˆ™å¿½ç•¥
            # æ£€æŸ¥åŸå¸‚åæ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
            import re
            has_chinese = bool(re.search(r'[\u4e00-\u9fff]', city))
            
            # å¦‚æœåŸå¸‚åæ˜¯çº¯è‹±æ–‡ï¼Œåªæ˜¾ç¤ºçœä»½
            if not has_chinese and province:
                display_city = province
                display_address = province
            elif city and has_chinese:
                display_city = city
                display_address = f"{province} {city}".strip()
            else:
                display_city = province or 'æœªçŸ¥'
                display_address = province or 'æœªçŸ¥'
            
            logger.info(f"åœ°åå¤„ç†: province={province}, city={city}, has_chinese={has_chinese}, display={display_address}")
            
            # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
            result = {
                'status': 0,
                'message': 'success',
                'content': {
                    'address': display_address,
                    'address_detail': {
                        'province': province,
                        'city': display_city,
                        'district': district,
                        'city_code': '',
                    },
                    'point': {
                        'x': str(data.get('lon', '')),
                        'y': str(data.get('lat', ''))
                    }
                },
                'ip': data.get('query', ''),
                'source': 'ip-api.com'
            }
            logger.info(f"âœ… å®šä½æˆåŠŸ: {result['content']['address']}")
            return jsonify(result), 200
        else:
            error_msg = data.get('message', 'å®šä½å¤±è´¥')
            logger.error(f"IPå®šä½å¤±è´¥: {error_msg}")
            return jsonify({'status': -1, 'message': error_msg}), 400
        
    except requests.Timeout:
        logger.error("IPå®šä½è¯·æ±‚è¶…æ—¶")
        return jsonify({'status': -1, 'message': 'è¯·æ±‚è¶…æ—¶'}), 504
        
    except requests.RequestException as e:
        logger.error(f"IPå®šä½è¯·æ±‚å¤±è´¥: {str(e)}")
        return jsonify({'status': -1, 'message': f'è¯·æ±‚å¤±è´¥: {str(e)}'}), 500
        
    except Exception as e:
        logger.error(f"IPå®šä½æœªçŸ¥é”™è¯¯: {str(e)}")
        return jsonify({'status': -1, 'message': f'æœåŠ¡å™¨é”™è¯¯: {str(e)}'}), 500

@app.route('/api/search-notes', methods=['GET'])
def search_notes():
    """
    æœç´¢ç¾é£Ÿç¬”è®°æ¥å£
    
    å‚æ•°ï¼š
    - query: æœç´¢å…³é”®è¯ï¼ˆå¿…å¡«ï¼‰
    - page: é¡µç ï¼Œé»˜è®¤1
    
    è¿”å›ï¼šç™¾åº¦æœç´¢ç»“æœçš„HTML
    """
    try:
        # è·å–è¯·æ±‚å‚æ•°
        query = request.args.get('query', '').strip()
        page = int(request.args.get('page', 1))
        
        if not query:
            return jsonify({'error': 'æœç´¢å…³é”®è¯ä¸èƒ½ä¸ºç©º'}), 400
        
        # æ„å»ºç™¾åº¦æœç´¢URL
        # pd=note å‚æ•°ç”¨äºæœç´¢ç¬”è®°ç±»å†…å®¹
        # pn å‚æ•°ç”¨äºåˆ†é¡µï¼Œæ¯é¡µ10æ¡ç»“æœ
        baidu_url = f"https://www.baidu.com/s?wd={query}&pd=note&rpf=pc&pn={(page-1)*10}"
        
        logger.info(f"æœç´¢è¯·æ±‚: query={query}, page={page}")
        logger.info(f"ç™¾åº¦URL: {baidu_url}")
        
        # åˆ›å»ºsessionä»¥æ”¯æŒcookie
        session = requests.Session()
        
        # å‘èµ·è¯·æ±‚
        # requestsä¼šè‡ªåŠ¨å¤„ç†gzipè§£å‹å’Œç¼–ç 
        response = session.get(
            baidu_url,
            headers=get_headers('https://www.baidu.com/', 'baidu'),
            timeout=10,
            allow_redirects=True
        )
        
        # æ£€æŸ¥å“åº”çŠ¶æ€
        response.raise_for_status()
        
        # è®©requestsè‡ªåŠ¨å¤„ç†è§£ç 
        # response.textä¼šï¼š1) è‡ªåŠ¨è§£å‹gzip  2) æ ¹æ®Content-Typeè‡ªåŠ¨è§£ç 
        text_content = response.text
        
        logger.info(f"è¯·æ±‚æˆåŠŸ: status={response.status_code}, encoding={response.encoding}, length={len(text_content)} å­—ç¬¦")
        
        # æ£€æµ‹æ˜¯å¦è§¦å‘å®‰å…¨éªŒè¯
        if 'ç™¾åº¦å®‰å…¨éªŒè¯' in text_content or 'mkdjump' in text_content:
            logger.warning("âš ï¸ è§¦å‘ç™¾åº¦å®‰å…¨éªŒè¯")
            return jsonify({
                'error': 'è§¦å‘ç™¾åº¦å®‰å…¨éªŒè¯',
                'message': 'ç™¾åº¦æ£€æµ‹åˆ°è‡ªåŠ¨åŒ–è¯·æ±‚ï¼Œè¯·ç¨åé‡è¯•',
                'tips': [
                    'è¿™æ˜¯ç™¾åº¦çš„åçˆ¬è™«æœºåˆ¶ï¼Œå±äºæ­£å¸¸ç°è±¡',
                    'è¯·ç­‰å¾…1-2åˆ†é’Ÿåé‡è¯•',
                    'æˆ–è€…ç›´æ¥åœ¨æµè§ˆå™¨ä¸­è®¿é—®ç™¾åº¦æœç´¢'
                ]
            }), 403
        
        # è¿”å›HTMLå†…å®¹ï¼ˆç§»é™¤åŒ…å«ä¸­æ–‡çš„è‡ªå®šä¹‰å“åº”å¤´ï¼Œé¿å…ç¼–ç é”™è¯¯ï¼‰
        return text_content, 200, {
            'Content-Type': 'text/html; charset=utf-8'
        }
        
    except requests.Timeout:
        logger.error("è¯·æ±‚è¶…æ—¶")
        return jsonify({'error': 'è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•'}), 504
        
    except requests.RequestException as e:
        logger.error(f"è¯·æ±‚å¤±è´¥: {str(e)}")
        return jsonify({'error': f'è¯·æ±‚å¤±è´¥: {str(e)}'}), 500
        
    except ValueError as e:
        logger.error(f"å‚æ•°é”™è¯¯: {str(e)}")
        return jsonify({'error': 'é¡µç å‚æ•°å¿…é¡»æ˜¯æ•°å­—'}), 400
        
    except Exception as e:
        logger.error(f"æœªçŸ¥é”™è¯¯: {str(e)}")
        return jsonify({'error': f'æœåŠ¡å™¨é”™è¯¯: {str(e)}'}), 500

@app.route('/api/search-recipes', methods=['GET'])
def search_recipes():
    """
    æœç´¢è±†æœç¾é£Ÿèœè°±æ¥å£
    
    å‚æ•°ï¼š
    - query: æœç´¢å…³é”®è¯ï¼ˆå¿…å¡«ï¼‰
    - page: é¡µç ï¼Œé»˜è®¤1
    
    è¿”å›ï¼šè±†æœç¾é£Ÿæœç´¢ç»“æœçš„HTML
    """
    try:
        query = request.args.get('query', '').strip()
        page = int(request.args.get('page', 1))
        
        if not query:
            return jsonify({'error': 'æœç´¢å…³é”®è¯ä¸èƒ½ä¸ºç©º'}), 400
        
        # è±†æœç¾é£Ÿæœç´¢URL - ä½¿ç”¨caipuæœç´¢è€Œä¸æ˜¯search/recipe
        # æ ¼å¼ï¼šhttps://www.douguo.com/caipu/å…³é”®è¯
        from urllib.parse import quote
        encoded_query = quote(query)
        douguo_url = f"https://www.douguo.com/caipu/{encoded_query}"
        
        logger.info(f"æœç´¢èœè°±: query={query}, page={page}")
        logger.info(f"è±†æœURL: {douguo_url}")
        
        # åˆ›å»ºsessionå¹¶è®¾ç½®cookie
        session = requests.Session()
        
        # å…ˆè®¿é—®é¦–é¡µè·å–cookie
        try:
            session.get('https://www.douguo.com/', headers=get_headers('https://www.douguo.com/', 'douguo'), timeout=5)
        except:
            pass
        
        # å†è®¿é—®æœç´¢é¡µ
        response = session.get(
            douguo_url,
            headers=get_headers('https://www.douguo.com/', 'douguo'),
            timeout=15,
            allow_redirects=True
        )
        
        response.raise_for_status()
        text_content = response.text
        
        logger.info(f"è¯·æ±‚æˆåŠŸ: status={response.status_code}, length={len(text_content)} å­—ç¬¦")
        
        return text_content, 200, {
            'Content-Type': 'text/html; charset=utf-8'
        }
        
    except requests.Timeout:
        logger.error("è¯·æ±‚è¶…æ—¶")
        return jsonify({'error': 'è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•'}), 504
        
    except requests.RequestException as e:
        logger.error(f"è¯·æ±‚å¤±è´¥: {str(e)}")
        return jsonify({'error': f'è¯·æ±‚å¤±è´¥: {str(e)}'}), 500
        
    except ValueError as e:
        logger.error(f"å‚æ•°é”™è¯¯: {str(e)}")
        return jsonify({'error': 'é¡µç å‚æ•°å¿…é¡»æ˜¯æ•°å­—'}), 400
        
    except Exception as e:
        logger.error(f"æœªçŸ¥é”™è¯¯: {str(e)}")
        return jsonify({'error': f'æœåŠ¡å™¨é”™è¯¯: {str(e)}'}), 500

@app.route('/api/featured-recipes', methods=['GET'])
def featured_recipes():
    """
    è·å–è±†æœç¾é£Ÿç²¾é€‰æ¨èèœè°±
    
    è¿”å›ï¼šè±†æœç¾é£Ÿé¦–é¡µç²¾é€‰èœè°±çš„HTML
    """
    try:
        douguo_url = "https://www.douguo.com/"
        
        logger.info("è·å–ç²¾é€‰æ¨èèœè°±")
        
        session = requests.Session()
        response = session.get(
            douguo_url,
            headers=get_headers('https://www.douguo.com/', 'douguo'),
            timeout=15,
            allow_redirects=True
        )
        
        response.raise_for_status()
        text_content = response.text
        
        logger.info(f"è¯·æ±‚æˆåŠŸ: status={response.status_code}, length={len(text_content)} å­—ç¬¦")
        
        return text_content, 200, {
            'Content-Type': 'text/html; charset=utf-8'
        }
        
    except Exception as e:
        logger.error(f"è·å–ç²¾é€‰èœè°±å¤±è´¥: {str(e)}")
        return jsonify({'error': f'æœåŠ¡å™¨é”™è¯¯: {str(e)}'}), 500

@app.route('/api/health-recipes', methods=['GET'])
def health_recipes():
    """
    è·å–é¥®é£Ÿå¥åº·ç›¸å…³èœè°±
    
    å‚æ•°ï¼š
    - category: å¥åº·åˆ†ç±»ï¼ˆå¦‚ï¼šå‡è‚¥ã€å…»ç”Ÿã€è¡¥é’™ç­‰ï¼‰ï¼Œå¯é€‰
    
    è¿”å›ï¼šè±†æœç¾é£Ÿé¥®é£Ÿå¥åº·é¡µé¢çš„HTML
    """
    try:
        from urllib.parse import quote
        category = request.args.get('category', '').strip()
        
        # åˆ†ç±»æ˜ å°„ - æ˜ å°„åˆ°è±†æœç¾é£Ÿå®˜æ–¹åˆ†ç±»
        # è±†æœåˆ†ç±»æ¥æºï¼š
        # - é¥®é£Ÿå¥åº·ï¼šé¥®é£Ÿæ–°é—» ç¾å®¹ç˜¦èº« é¥®é£Ÿå°å¸¸è¯† å…»ç”Ÿç§˜æ–¹
        # - åŠŸèƒ½æ€§è°ƒç†ï¼šæ¸…çƒ­å»ç« å‡è‚¥ ç¥›ç—° ä¹Œå‘ æ»‹é˜´å£®é˜³ å¥è„¾å…»èƒƒ
        # - äººç¾¤è†³é£Ÿï¼šå­•å¦‡ è€äºº äº§å¦‡ å“ºä¹³æœŸ
        # - ç–¾ç—…è°ƒç†ï¼šç³–å°¿ç—… é«˜è¡€å‹ ç—›é£
        # - åŠŸæ•ˆè¥å…»ï¼šè¡¥é’™ è´«è¡€ æé«˜å…ç–«åŠ› å…»èƒƒ é˜²é›¾éœ¾ æ¶¦è‚ºæ­¢å’³ å…»é¢œ å¤±çœ  æŠ—ç™Œ
        category_mapping = {
            'å‡è‚¥': 'ç¾å®¹ç˜¦èº«',      # é¥®é£Ÿå¥åº· -> ç¾å®¹ç˜¦èº«ï¼ˆåŒ…å«å‡è‚¥ï¼‰
            'ç¾å®¹': 'å…»é¢œ',          # åŠŸæ•ˆè¥å…» -> å…»é¢œ
            'å¥è„¾': 'å¥è„¾å…»èƒƒ',      # åŠŸèƒ½æ€§è°ƒç† -> å¥è„¾å…»èƒƒ
            'è¡¥é’™': 'è¡¥é’™',          # åŠŸæ•ˆè¥å…» -> è¡¥é’™
            'æé«˜å…ç–«åŠ›': 'æé«˜å…ç–«åŠ›',  # åŠŸæ•ˆè¥å…» -> æé«˜å…ç–«åŠ›
            'æ¸…çƒ­': 'æ¸…çƒ­å»ç«',      # åŠŸèƒ½æ€§è°ƒç† -> æ¸…çƒ­å»ç«
            'æ¶¦è‚º': 'æ¶¦è‚ºæ­¢å’³',      # åŠŸæ•ˆè¥å…» -> æ¶¦è‚ºæ­¢å’³
            'ç³–å°¿ç—…': 'ç³–å°¿ç—…',      # ç–¾ç—…è°ƒç† -> ç³–å°¿ç—…
            'é«˜è¡€å‹': 'é«˜è¡€å‹'       # ç–¾ç—…è°ƒç† -> é«˜è¡€å‹
        }
        
        if category:
            # ä½¿ç”¨æ˜ å°„åçš„è±†æœå®˜æ–¹åˆ†ç±»å
            mapped_category = category_mapping.get(category, category)
            encoded_category = quote(mapped_category)
            # ä½¿ç”¨åˆ†ç±»é¡µé¢URLï¼ˆä¸æ˜¯æœç´¢æ¥å£ï¼‰
            douguo_url = f"https://www.douguo.com/caipu/{encoded_category}"
        else:
            # é»˜è®¤æ˜¾ç¤ºç²¾é€‰
            douguo_url = "https://www.douguo.com/jingxuan/home"
        
        logger.info(f"è·å–é¥®é£Ÿå¥åº·: category={category or 'ç²¾é€‰'}, mapped={category_mapping.get(category, category) if category else 'ç²¾é€‰'}, url={douguo_url}")
        
        session = requests.Session()
        
        # å…ˆè®¿é—®é¦–é¡µè·å–cookie
        try:
            session.get('https://www.douguo.com/', headers=get_headers('https://www.douguo.com/', 'douguo'), timeout=5)
        except:
            pass
        
        response = session.get(
            douguo_url,
            headers=get_headers('https://www.douguo.com/', 'douguo'),
            timeout=15,
            allow_redirects=True
        )
        
        response.raise_for_status()
        text_content = response.text
        
        logger.info(f"è¯·æ±‚æˆåŠŸ: status={response.status_code}, length={len(text_content)} å­—ç¬¦")
        
        return text_content, 200, {
            'Content-Type': 'text/html; charset=utf-8'
        }
        
    except Exception as e:
        logger.error(f"è·å–é¥®é£Ÿå¥åº·å¤±è´¥: {str(e)}")
        return jsonify({'error': f'æœåŠ¡å™¨é”™è¯¯: {str(e)}'}), 500

@app.route('/api/note-detail', methods=['GET'])
def note_detail():
    """
    è·å–ç¬”è®°è¯¦æƒ…æ¥å£ - æ”¯æŒå¤šç§æ¥æºçš„å·®å¼‚åŒ–å¤„ç†
    
    å‚æ•°ï¼š
    - url: ç¬”è®°è¯¦æƒ…é¡µURLï¼ˆå¿…å¡«ï¼‰
    - debug: æ˜¯å¦è¿”å›è°ƒè¯•ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
    
    è¿”å›ï¼šJSONæ ¼å¼çš„ç¬”è®°è¯¦æƒ…
    """
    try:
        note_url = request.args.get('url', '').strip()
        debug_mode = request.args.get('debug', '').lower() == 'true'
        
        if not note_url:
            return jsonify({'error': 'ç¬”è®°URLä¸èƒ½ä¸ºç©º'}), 400
        
        logger.info(f"è·å–ç¬”è®°è¯¦æƒ…: url={note_url}, debug={debug_mode}")
        
        session = requests.Session()
        
        # å…ˆè®¿é—®ç™¾åº¦é¦–é¡µè·å–cookie
        try:
            time.sleep(0.5)
            session.get('https://www.baidu.com/', headers=get_headers('https://www.baidu.com/', 'baidu'), timeout=10)
            time.sleep(1)
        except Exception as e:
            logger.warning(f"è®¿é—®ç™¾åº¦é¦–é¡µå¤±è´¥: {e}")
        
        # è®¿é—®è¯¦æƒ…é¡µ
        response = session.get(
            note_url,
            headers=get_headers('https://www.baidu.com/', 'baidu'),
            timeout=15,
            allow_redirects=True
        )
        
        text_content = response.text
        logger.info(f"è¯·æ±‚å®Œæˆ: status={response.status_code}, length={len(text_content)} å­—ç¬¦")
        
        # æ£€æµ‹æ˜¯å¦æ˜¯ç™¾åº¦çš„è·³è½¬é¡µé¢
        import re
        import json
        from bs4 import BeautifulSoup
        
        real_url = None
        is_baidu_redirect = False  # æ ‡è®°æ˜¯å¦æ˜¯ç™¾åº¦è·³è½¬
        
        # æ£€æµ‹æ˜¯å¦æ˜¯ç™¾åº¦çš„è·³è½¬é¡µé¢ï¼ˆåˆ¤æ–­åŸå§‹URLæ˜¯å¦åŒ…å«baidu.comï¼‰
        if 'baidu.com' in note_url or 'm.baidu.com' in note_url:
            is_baidu_redirect = True
        
        # æ–¹æ³•1: æ£€æŸ¥ window.location.replace
        match = re.search(r'window\.location\.replace\(["\']([^"\']+)["\']\)', text_content)
        if match:
            real_url = match.group(1)
            logger.info(f"æ£€æµ‹åˆ°JavaScriptè·³è½¬: {real_url}")
        
        # æ–¹æ³•2: æ£€æŸ¥ meta refresh
        if not real_url:
            match = re.search(r'<meta[^>]+http-equiv=["\']refresh["\'][^>]+content=["\'][^;]+;\s*url=([^"\']+)["\']', text_content, re.IGNORECASE)
            if match:
                real_url = match.group(1)
                logger.info(f"æ£€æµ‹åˆ°meta refreshè·³è½¬: {real_url}")
        
        # å¦‚æœæ£€æµ‹åˆ°è·³è½¬ï¼Œé‡æ–°è¯·æ±‚çœŸå®URL
        if real_url:
            logger.info(f"è·³è½¬åˆ°çœŸå®URL: {real_url}")
            time.sleep(1)  # å»¶è¿Ÿ1ç§’
            
            # æ ¹æ®ç›®æ ‡åŸŸåè®¾ç½®åˆé€‚çš„headers
            if 'baidu.com' in real_url or 'mbd.baidu.com' in real_url:
                real_headers = get_headers('https://www.baidu.com/', 'baidu')
            else:
                # å¯¹äºç¬¬ä¸‰æ–¹ç½‘ç«™ï¼Œä½¿ç”¨é€šç”¨headers
                real_headers = {
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
                    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                    'Accept-Encoding': 'gzip, deflate',
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1',
                }
            
            response = session.get(
                real_url,
                headers=real_headers,
                timeout=15,
                allow_redirects=True
            )
            
            text_content = response.text
            logger.info(f"çœŸå®é¡µé¢è¯·æ±‚å®Œæˆ: status={response.status_code}, length={len(text_content)} å­—ç¬¦, URL={real_url}")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯é¡µé¢
        if response.status_code >= 400:
            logger.error(f"æœåŠ¡å™¨è¿”å›é”™è¯¯: {response.status_code}")
            return jsonify({'error': f'æœåŠ¡å™¨è¿”å›é”™è¯¯: {response.status_code}', 'type': 'error'}), response.status_code
        
        # æ ¹æ®çœŸå®URLåˆ¤æ–­æ¥æºç±»å‹å¹¶è§£æ
        result = {
            'type': 'unknown',
            'title': '',
            'content': '',
            'images': [],
            'source': '',
            'publishTime': '',
            'rawUrl': real_url or note_url,
            'originalUrl': note_url  # ä¿ç•™åŸå§‹URL
        }
        
        # å¦‚æœæ˜¯ç™¾åº¦è·³è½¬åˆ°ç¬¬ä¸‰æ–¹ç½‘ç«™ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
        if is_baidu_redirect and real_url and 'baidu.com' not in real_url:
            logger.info(f"ç™¾åº¦è·³è½¬åˆ°ç¬¬ä¸‰æ–¹: {real_url}")
            
            # å¯¹äºç™¾åº¦è·³è½¬åˆ°ç¬¬ä¸‰æ–¹çš„æƒ…å†µï¼Œæ ¹æ®ç›®æ ‡ç½‘ç«™ç±»å‹å¤„ç†
            if 'm.dianping.com' in real_url:
                # å¤§ä¼—ç‚¹è¯„å¯ä»¥è§£æ
                pass  # ç»§ç»­ä¸‹é¢çš„æ­£å¸¸æµç¨‹
            else:
                # å…¶ä»–ç¬¬ä¸‰æ–¹ç½‘ç«™ï¼ˆæºç¨‹ã€å°çº¢ä¹¦ç­‰ï¼‰ï¼Œæç¤ºè·³è½¬
                result['type'] = 'baidu_redirect'
                result['source'] = 'ç™¾åº¦ç¬”è®°'
                
                # å°è¯•ä»ç›®æ ‡ç½‘ç«™æå–åŸºæœ¬ä¿¡æ¯
                soup = BeautifulSoup(text_content, 'html.parser')
                title_tag = soup.find('title')
                if title_tag:
                    result['title'] = title_tag.get_text(strip=True)
                
                # è¯†åˆ«ç›®æ ‡ç½‘ç«™
                if 'ctrip.com' in real_url:
                    result['content'] = 'è¯¥ç¬”è®°æ¥è‡ªæºç¨‹æ—…è¡Œï¼Œå†…å®¹ä¸°å¯Œå¤šæ ·ã€‚'
                    result['source'] = 'ç™¾åº¦ç¬”è®° â†’ æºç¨‹æ—…è¡Œ'
                elif 'xiaohongshu.com' in real_url or 'xhslink.com' in real_url:
                    result['content'] = 'è¯¥ç¬”è®°æ¥è‡ªå°çº¢ä¹¦ï¼Œå†…å®¹ç²¾å½©çº·å‘ˆã€‚'
                    result['source'] = 'ç™¾åº¦ç¬”è®° â†’ å°çº¢ä¹¦'
                else:
                    result['content'] = 'è¯¥ç¬”è®°æ¥è‡ªç¬¬ä¸‰æ–¹ç½‘ç«™ã€‚'
                    result['source'] = 'ç™¾åº¦ç¬”è®°'
                
                result['needJump'] = True
                logger.info(f"ç™¾åº¦è·³è½¬ç¬¬ä¸‰æ–¹ï¼Œè¿”å›è·³è½¬æç¤º: {result['source']}")
                return jsonify(result), 200
        
        # ç±»å‹1: å¤§ä¼—ç‚¹è¯„ - ä»JSONæ•°æ®ä¸­æå–
        if real_url and 'm.dianping.com' in real_url:
            logger.info("æ£€æµ‹åˆ°å¤§ä¼—ç‚¹è¯„ç¬”è®°ï¼Œä»JSONæå–æ•°æ®")
            result['type'] = 'dianping'
            
            # æå– __NEXT_DATA__ ä¸­çš„JSONæ•°æ®
            match = re.search(r'<script id="__NEXT_DATA__" type="application/json"[^>]*>(.*?)</script>', text_content, re.DOTALL)
            if match:
                try:
                    data = json.loads(match.group(1))
                    feed_info = data.get('props', {}).get('pageProps', {}).get('feedInfo', {})
                    
                    result['title'] = feed_info.get('title', '')
                    result['content'] = feed_info.get('content', '')
                    
                    # å¦‚æœæ˜¯ä»ç™¾åº¦è·³è½¬æ¥çš„ï¼Œåœ¨æ¥æºä¸­æ ‡æ³¨
                    author = feed_info.get('feedUser', {}).get('nickName', 'å¤§ä¼—ç‚¹è¯„ç”¨æˆ·')
                    if is_baidu_redirect:
                        result['source'] = f'{author} (ç™¾åº¦ç¬”è®° â†’ å¤§ä¼—ç‚¹è¯„)'
                    else:
                        result['source'] = author
                    
                    # æå–å›¾ç‰‡
                    pic_list = feed_info.get('feedPicList', [])
                    result['images'] = [pic.get('url', '') for pic in pic_list if pic.get('url')]
                    
                    logger.info(f"å¤§ä¼—ç‚¹è¯„æ•°æ®æå–æˆåŠŸ: title={result['title']}, content_length={len(result.get('content', ''))}, images={len(result['images'])}")
                except Exception as e:
                    logger.error(f"è§£æå¤§ä¼—ç‚¹è¯„JSONå¤±è´¥: {e}")
                    result['type'] = 'parse_error'
                    result['error'] = f'è§£æå¤±è´¥: {str(e)}'
            else:
                logger.warning("æœªæ‰¾åˆ°å¤§ä¼—ç‚¹è¯„çš„JSONæ•°æ®")
                result['error'] = 'æ— æ³•æå–ç¬”è®°å†…å®¹'
                result['needJump'] = True
        
        # ç±»å‹2: æºç¨‹æ—…è¡Œ - ä»JSONæ•°æ®ä¸­æå–
        elif real_url and 'm.ctrip.com' in real_url:
            logger.info("æ£€æµ‹åˆ°æºç¨‹æ—…è¡Œç¬”è®°ï¼Œä»JSONæå–æ•°æ®")
            result['type'] = 'ctrip'
            result['source'] = 'æºç¨‹æ—…è¡Œ'
            
            # æºç¨‹é¡µé¢çš„æ•°æ®åœ¨ __NEXT_DATA__ ä¸­ï¼Œä½†ç»“æ„ä¸åŒ
            # è¿™ä¸ªé¡µé¢ä¸»è¦æ˜¯ä¸€ä¸ªNext.jsåº”ç”¨ï¼Œå†…å®¹æ˜¯å®¢æˆ·ç«¯æ¸²æŸ“çš„
            # å°è¯•æå–åŸºæœ¬ä¿¡æ¯
            soup = BeautifulSoup(text_content, 'html.parser')
            
            # æºç¨‹çš„é¡µé¢æ ‡é¢˜
            title_tag = soup.find('title')
            if title_tag:
                result['title'] = title_tag.string or ''
            
            # å°è¯•ä»metaæ ‡ç­¾è·å–æè¿°
            desc_tag = soup.find('meta', {'name': 'description'})
            if desc_tag and desc_tag.get('content'):
                result['content'] = desc_tag.get('content')
            
            # æºç¨‹é¡µé¢ä¸»è¦æ˜¯å®¢æˆ·ç«¯æ¸²æŸ“ï¼ŒæœåŠ¡ç«¯HTMLä¸­å†…å®¹è¾ƒå°‘
            # å»ºè®®è·³è½¬åˆ°åŸç½‘ç«™æŸ¥çœ‹å®Œæ•´ä½“éªŒ
            if not result['content']:
                result['content'] = 'è¯¥ç¬”è®°æ¥è‡ªæºç¨‹æ—…è¡Œï¼Œä¸ºäº†è·å¾—æœ€ä½³ä½“éªŒï¼Œå»ºè®®å‰å¾€åŸç½‘ç«™æŸ¥çœ‹ã€‚'
            result['needJump'] = True
            
        # ç±»å‹3: ç™¾åº¦ç¬”è®° - å°è¯•ç»•è¿‡å®‰å…¨éªŒè¯
        elif real_url and 'mbd.baidu.com' in real_url:
            logger.info("æ£€æµ‹åˆ°ç™¾åº¦ç¬”è®°ï¼Œå°è¯•è·å–å†…å®¹")
            result['type'] = 'baidu'
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å®‰å…¨éªŒè¯é¡µé¢
            if 'å®‰å…¨éªŒè¯' in text_content or 'timeout' in text_content or len(text_content) < 2000:
                logger.warning("é‡åˆ°ç™¾åº¦å®‰å…¨éªŒè¯æˆ–å†…å®¹è¿‡çŸ­")
                
                # å°è¯•é‡æ–°è¯·æ±‚ï¼Œæ·»åŠ æ›´å¤šå»¶è¿Ÿå’ŒçœŸå®æµè§ˆå™¨ç‰¹å¾
                try:
                    time.sleep(2)  # å¢åŠ å»¶è¿Ÿåˆ°2ç§’
                    
                    # ä½¿ç”¨æ›´å®Œæ•´çš„æµè§ˆå™¨headers
                    retry_headers = {
                        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
                        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                        'Accept-Encoding': 'gzip, deflate',
                        'Connection': 'keep-alive',
                        'Upgrade-Insecure-Requests': '1',
                        'Sec-Fetch-Dest': 'document',
                        'Sec-Fetch-Mode': 'navigate',
                        'Sec-Fetch-Site': 'none',
                        'Sec-Fetch-User': '?1',
                        'Cache-Control': 'max-age=0',
                        'Referer': 'https://www.baidu.com/',
                    }
                    
                    retry_response = session.get(
                        real_url,
                        headers=retry_headers,
                        timeout=15,
                        allow_redirects=True
                    )
                    
                    if retry_response.status_code == 200:
                        text_content = retry_response.text
                        logger.info(f"é‡è¯•æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(text_content)}")
                except Exception as retry_error:
                    logger.error(f"é‡è¯•å¤±è´¥: {retry_error}")
                
                # å†æ¬¡æ£€æŸ¥
                if 'å®‰å…¨éªŒè¯' in text_content or 'timeout' in text_content or len(text_content) < 2000:
                    result['type'] = 'security_check'
                    result['error'] = 'è¯¥ç¬”è®°éœ€è¦é€šè¿‡ç™¾åº¦å®‰å…¨éªŒè¯æ‰èƒ½æŸ¥çœ‹'
                    result['needJump'] = True
                    return jsonify(result), 200
            
            # å°è¯•è§£æç™¾åº¦ç¬”è®°å†…å®¹
            soup = BeautifulSoup(text_content, 'html.parser')
            
            # æå–æ ‡é¢˜
            title_tag = soup.find('h1') or soup.find('title')
            if title_tag:
                result['title'] = title_tag.get_text(strip=True) if hasattr(title_tag, 'get_text') else title_tag.string
            
            result['source'] = 'ç™¾åº¦ç¬”è®°'
            
            # å°è¯•æå–æ­£æ–‡å†…å®¹
            content_selectors = [
                '.content-article',
                '.article-content',
                '[class*="content"]',
                'article',
                '.detail-content'
            ]
            
            for selector in content_selectors:
                content_el = soup.select_one(selector)
                if content_el:
                    paragraphs = content_el.find_all('p')
                    if paragraphs:
                        result['content'] = '\n\n'.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
                        break
                    elif content_el.get_text(strip=True):
                        result['content'] = content_el.get_text(strip=True)
                        break
            
            # æå–å›¾ç‰‡
            images = soup.find_all('img')
            for img in images:
                src = img.get('src') or img.get('data-src') or img.get('data-original')
                if src and not any(x in src for x in ['icon', 'logo', 'avatar']):
                    # å¤„ç†ç›¸å¯¹URL
                    if src.startswith('//'):
                        src = 'https:' + src
                    elif src.startswith('/'):
                        src = 'https://mbd.baidu.com' + src
                    result['images'].append(src)
            
            # å¦‚æœæå–å¤±è´¥ï¼Œæ ‡è®°ä¸ºéœ€è¦è·³è½¬
            if not result['title'] and not result['content']:
                result['needJump'] = True
                result['error'] = 'å†…å®¹è§£æå¤±è´¥ï¼Œå»ºè®®å‰å¾€åŸç½‘ç«™æŸ¥çœ‹'
        
        # æœªçŸ¥ç±»å‹ - é€šç”¨HTMLè§£æ
        else:
            logger.info("æœªçŸ¥æ¥æºï¼Œä½¿ç”¨é€šç”¨HTMLè§£æ")
            result['type'] = 'generic'
            soup = BeautifulSoup(text_content, 'html.parser')
            
            # å°è¯•æå–æ ‡é¢˜
            title_el = soup.find('h1') or soup.find(class_='title')
            if title_el:
                result['title'] = title_el.get_text(strip=True)
            
            # å°è¯•æå–å†…å®¹
            content_selectors = ['.content', '.article-content', 'article', '.detail-content']
            for selector in content_selectors:
                content_el = soup.select_one(selector)
                if content_el:
                    paragraphs = content_el.find_all('p')
                    result['content'] = '\n\n'.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
                    if result['content']:
                        break
        
        # æœ€ç»ˆç»“æœæ—¥å¿—
        logger.info(f"ğŸ“¤ è¿”å›ç¬”è®°è¯¦æƒ…: type={result.get('type')}, has_title={bool(result.get('title'))}, has_content={bool(result.get('content'))}, images_count={len(result.get('images', []))}, needJump={result.get('needJump', False)}")
        
        return jsonify(result), 200
        
    except requests.Timeout:
        logger.error("è¯·æ±‚è¶…æ—¶")
        return jsonify({'error': 'è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•', 'type': 'timeout'}), 504
        
    except requests.RequestException as e:
        logger.error(f"è¯·æ±‚å¤±è´¥: {str(e)}")
        return jsonify({'error': f'è¯·æ±‚å¤±è´¥: {str(e)}', 'type': 'request_error'}), 500
        
    except Exception as e:
        logger.error(f"æœªçŸ¥é”™è¯¯: {str(e)}")
        return jsonify({'error': f'æœåŠ¡å™¨é”™è¯¯: {str(e)}', 'type': 'server_error'}), 500

@app.route('/api/recipe-detail', methods=['GET'])
def recipe_detail():
    """
    è·å–èœè°±è¯¦æƒ…æ¥å£
    
    å‚æ•°ï¼š
    - url: èœè°±è¯¦æƒ…é¡µURLï¼ˆå¿…å¡«ï¼‰
    - debug: æ˜¯å¦è¿”å›è°ƒè¯•ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
    
    è¿”å›ï¼šèœè°±è¯¦æƒ…çš„HTML
    """
    try:
        recipe_url = request.args.get('url', '').strip()
        debug_mode = request.args.get('debug', '').lower() == 'true'
        
        if not recipe_url:
            return jsonify({'error': 'èœè°±URLä¸èƒ½ä¸ºç©º'}), 400
        
        # ç¡®ä¿URLæ˜¯å®Œæ•´çš„
        if not recipe_url.startswith('http'):
            recipe_url = f"https://www.douguo.com{recipe_url}"
        
        logger.info(f"è·å–èœè°±è¯¦æƒ…: url={recipe_url}, debug={debug_mode}")
        
        session = requests.Session()
        
        # å…ˆè®¿é—®é¦–é¡µè·å–cookie
        try:
            time.sleep(0.5)  # å»¶è¿ŸåŠç§’
            session.get('https://www.douguo.com/', headers=get_headers('https://www.douguo.com/', 'douguo'), timeout=10)
            time.sleep(1)  # å»¶è¿Ÿ1ç§’ï¼Œæ¨¡æ‹ŸçœŸå®ç”¨æˆ·
        except Exception as e:
            logger.warning(f"è®¿é—®é¦–é¡µå¤±è´¥: {e}")
        
        # è®¿é—®è¯¦æƒ…é¡µ
        response = session.get(
            recipe_url,
            headers=get_headers('https://www.douguo.com/', 'douguo'),
            timeout=15,
            allow_redirects=True
        )
        
        text_content = response.text
        logger.info(f"è¯·æ±‚å®Œæˆ: status={response.status_code}, length={len(text_content)} å­—ç¬¦")
        
        # è°ƒè¯•æ¨¡å¼ï¼šåœ¨HTMLä¸­æ·»åŠ æ³¨é‡Šæ˜¾ç¤ºå…³é”®ç»“æ„
        if debug_mode:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(text_content, 'html.parser')
            
            debug_info = f"""
<!-- ===== è°ƒè¯•ä¿¡æ¯ ===== 
URL: {recipe_url}
çŠ¶æ€ç : {response.status_code}
å†…å®¹é•¿åº¦: {len(text_content)}
Title: {soup.title.string if soup.title else 'æœªæ‰¾åˆ°'}

æ‰¾åˆ°çš„ä¸»è¦å…ƒç´ :
- h1: {len(soup.find_all('h1'))} ä¸ª
- .title: {len(soup.select('.title'))} ä¸ª
- img: {len(soup.find_all('img'))} ä¸ª
- .ings li: {len(soup.select('.ings li'))} ä¸ª
- .steps li: {len(soup.select('.steps li'))} ä¸ª
- .cookstep: {len(soup.select('.cookstep'))} ä¸ª

å‰10ä¸ªdivçš„class:
{chr(10).join([f"  - {div.get('class', [])}" for div in soup.find_all('div', class_=True)[:10]])}
===== è°ƒè¯•ä¿¡æ¯ç»“æŸ ===== -->
"""
            text_content = debug_info + text_content
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯é¡µé¢
        if response.status_code >= 400:
            logger.error(f"æœåŠ¡å™¨è¿”å›é”™è¯¯: {response.status_code}")
            return jsonify({'error': f'æœåŠ¡å™¨è¿”å›é”™è¯¯: {response.status_code}', 'html': text_content[:500]}), response.status_code
        
        return text_content, 200, {
            'Content-Type': 'text/html; charset=utf-8'
        }
        
    except requests.Timeout:
        logger.error("è¯·æ±‚è¶…æ—¶")
        return jsonify({'error': 'è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•'}), 504
        
    except requests.RequestException as e:
        logger.error(f"è¯·æ±‚å¤±è´¥: {str(e)}")
        return jsonify({'error': f'è¯·æ±‚å¤±è´¥: {str(e)}'}), 500
        
    except Exception as e:
        logger.error(f"æœªçŸ¥é”™è¯¯: {str(e)}")
        return jsonify({'error': f'æœåŠ¡å™¨é”™è¯¯: {str(e)}'}), 500

@app.errorhandler(404)
def not_found(error):
    """404é”™è¯¯å¤„ç†"""
    return jsonify({'error': 'æ¥å£ä¸å­˜åœ¨'}), 404

@app.errorhandler(500)
def internal_error(error):
    """500é”™è¯¯å¤„ç†"""
    return jsonify({'error': 'æœåŠ¡å™¨å†…éƒ¨é”™è¯¯'}), 500

# ============================================
# æ•°æ®åº“API - è¯»å–æœ¬åœ°ç¾é£Ÿæ•°æ®
# ============================================

def get_db_connection():
    """è·å–æ•°æ®åº“è¿æ¥"""
    # æ•°æ®åº“æ–‡ä»¶åœ¨é¡¹ç›®æ ¹ç›®å½•
    db_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'data.db')
    
    if not os.path.exists(db_path):
        raise FileNotFoundError(f'æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {db_path}')
    
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row  # ä½¿ç”¨Rowå·¥å‚ï¼Œå¯ä»¥é€šè¿‡åˆ—åè®¿é—®
    return conn

@app.route('/api/dishes', methods=['GET'])
def get_dishes():
    """
    è·å–èœå“æ•°æ®
    
    å‚æ•°ï¼š
    - limit: è¿”å›æ•°é‡ï¼Œé»˜è®¤1050
    - offset: åç§»é‡ï¼Œé»˜è®¤0
    - shop: åº—é“ºåç§°è¿‡æ»¤ï¼ˆå¯é€‰ï¼‰
    - sort: æ’åºæ–¹å¼ï¼Œé»˜è®¤recommendationï¼ˆæŒ‰æ¨èæ•°ï¼‰ï¼Œå¯é€‰nameï¼ˆæŒ‰åç§°ï¼‰
    
    è¿”å›ï¼šèœå“åˆ—è¡¨
    """
    try:
        limit = request.args.get('limit', 1050, type=int)
        offset = request.args.get('offset', 0, type=int)
        shop = request.args.get('shop', '', type=str)
        sort = request.args.get('sort', 'recommendation', type=str)
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # æ„å»ºSQLæŸ¥è¯¢
        sql = 'SELECT name, image_url, recommendation_count, shop_name FROM dishes'
        params = []
        
        # åº—é“ºè¿‡æ»¤
        if shop:
            sql += ' WHERE shop_name LIKE ?'
            params.append(f'%{shop}%')
        
        # æ’åº
        if sort == 'name':
            sql += ' ORDER BY name ASC'
        else:
            sql += ' ORDER BY recommendation_count DESC'
        
        # åˆ†é¡µ
        sql += ' LIMIT ? OFFSET ?'
        params.extend([limit, offset])
        
        cursor.execute(sql, params)
        rows = cursor.fetchall()
        
        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        dishes = []
        for row in rows:
            dishes.append({
                'èœå“åç§°': row['name'],
                'èœå“å›¾ç‰‡url': row['image_url'],
                'èœå“æ¨èäººæ•°': row['recommendation_count'],
                'åº—å': row['shop_name']
            })
        
        conn.close()
        
        logger.info(f'è¿”å›èœå“æ•°æ®: {len(dishes)}æ¡ (limit={limit}, offset={offset})')
        
        return jsonify({
            'success': True,
            'data': dishes,
            'count': len(dishes),
            'limit': limit,
            'offset': offset
        })
        
    except Exception as e:
        logger.error(f'è·å–èœå“æ•°æ®å¤±è´¥: {str(e)}')
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/shops', methods=['GET'])
def get_shops():
    """
    è·å–åº—é“ºæ•°æ®
    
    å‚æ•°ï¼š
    - limit: è¿”å›æ•°é‡ï¼Œé»˜è®¤200
    - offset: åç§»é‡ï¼Œé»˜è®¤0
    - sort: æ’åºæ–¹å¼ï¼Œé»˜è®¤scoreï¼ˆæŒ‰è¯„åˆ†ï¼‰ï¼Œå¯é€‰nameï¼ˆæŒ‰åç§°ï¼‰
    
    è¿”å›ï¼šåº—é“ºåˆ—è¡¨
    """
    try:
        limit = request.args.get('limit', 200, type=int)
        offset = request.args.get('offset', 0, type=int)
        sort = request.args.get('sort', 'score', type=str)
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # æ„å»ºSQLæŸ¥è¯¢
        sql = 'SELECT name, avg_price, address, phone, detail_url, score FROM shops'
        
        # æ’åº
        if sort == 'name':
            sql += ' ORDER BY name ASC'
        else:
            sql += ' ORDER BY score DESC NULLS LAST'
        
        # åˆ†é¡µ
        sql += ' LIMIT ? OFFSET ?'
        
        cursor.execute(sql, [limit, offset])
        rows = cursor.fetchall()
        
        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        shops = []
        for row in rows:
            shops.append({
                'åº—å': row['name'],
                'äººå‡æ¶ˆè´¹': row['avg_price'],
                'åœ°å€': row['address'],
                'ç”µè¯': row['phone'],
                'è¯¦æƒ…é¡µ': row['detail_url'],
                'è¯„åˆ†score': row['score'] if row['score'] is not None else ''
            })
        
        conn.close()
        
        logger.info(f'è¿”å›åº—é“ºæ•°æ®: {len(shops)}æ¡ (limit={limit}, offset={offset})')
        
        return jsonify({
            'success': True,
            'data': shops,
            'count': len(shops),
            'limit': limit,
            'offset': offset
        })
        
    except Exception as e:
        logger.error(f'è·å–åº—é“ºæ•°æ®å¤±è´¥: {str(e)}')
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

if __name__ == '__main__':
    print("=" * 60)
    print("ğŸš€ ç¾é£Ÿç¬”è®°æœç´¢APIæœåŠ¡å¯åŠ¨")
    print("=" * 60)
    print("ğŸ“ æœ¬åœ°åœ°å€: http://localhost:5000")
    print("ğŸ“ å¥åº·æ£€æŸ¥: http://localhost:5000/api/health")
    print("ğŸ“ æœç´¢æ¥å£: http://localhost:5000/api/search-notes?query=æ­å·ç¾é£Ÿæ¨è")
    print("ğŸ“ èœå“æ•°æ®: http://localhost:5000/api/dishes")
    print("ğŸ“ åº—é“ºæ•°æ®: http://localhost:5000/api/shops")
    print("=" * 60)
    print("\næŒ‰ Ctrl+C åœæ­¢æœåŠ¡\n")
    
    # å¯åŠ¨Flaskåº”ç”¨
    # debug=True: å¼€å‘æ¨¡å¼ï¼Œä»£ç ä¿®æ”¹åè‡ªåŠ¨é‡å¯
    # host='0.0.0.0': å…è®¸å¤–éƒ¨è®¿é—®
    # port=5000: ç«¯å£å·
    app.run(
        debug=True,
        host='0.0.0.0',
        port=5000
    )

